{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Z7r_zEm9HLocS1YvagL2HX08mQ-ZzesB",
      "authorship_tag": "ABX9TyOxNZ2qHq0GE3AGKwayGpwa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sirfindcent/food-vision/blob/main/Notebooks/01_food_vision_feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01. ðŸ”ðŸ‘ Food Vision Feature Extraction\n",
        "\n",
        "  In this notebook, we're going to start our experiment small, small dataset and small model. And we will gradually scale up if the experiments work.\n",
        "\n",
        "  ![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/06-ml-serial-experimentation.png)\n",
        "*Machine learning practitioners are serial experimenters. Start small, get a model working, see if your experiments work then gradually scale them up to where you want to go (we're going to be looking at scaling up throughout this notebook).*\n",
        "\n",
        "  \n",
        "\n",
        "We're going to go through the following:\n",
        "- Download and prepare the smaller dataset.\n",
        "- Build a transfer learning feature extraction model using TensorFlow Hub.\n",
        "- Train the model.\n",
        "- Compare model results using TensorBoard/\n",
        "- Testing the feature extraction model on custom images.\n",
        "\n",
        "## 1. Problem\n",
        "The main goal of this food vision project:\n",
        "> Given an image of food, can we identify the name of the food with a deep learning model?\n",
        "\n",
        "We will do some experiments to reach the goal. We're going to:\n",
        "1. Try the feature extraction model\n",
        "2. Try the fine-tuned model\n",
        "3. If all of them work, we will train the fine-tuned model on all data instead of smaller data.\n",
        "\n",
        "## 2. Data\n",
        "The data we're downloading comes from the original Food101 dataset but has been preprocessed with [image_data_modification notebook](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_data_modification.ipynb).\n",
        "\n",
        "In this notebook, we're going to use the smaller dataset and few classes, 10 food classes instead of 101.\n",
        "\n",
        "## 3. Features\n",
        "We'll be training on less data but evaluating our models on more test data.\n",
        "* There are only 750 training images (labelled images)\n",
        "* There are 2500 test images (unlabelled images)..\n",
        "\n",
        "More information about the data:\n",
        "* We're dealing with images (unstructured data) so it's probably best we use deep learning/transfer learning.\n",
        "* There are 10 food classes.\n"
      ],
      "metadata": {
        "id": "RR1Wncg_nvZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## How you can use this notebook\n",
        "\n",
        "You can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.\n",
        "\n",
        "Write all of the code yourself.\n",
        "\n",
        "Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?\n",
        "\n",
        "You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.\n",
        "\n",
        "Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to **write more code**."
      ],
      "metadata": {
        "id": "ID6sFrzVICSi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTWetPM7AWfY"
      },
      "source": [
        "## Using a GPU\n",
        "\n",
        "To begin, let's check to see if we're using a GPU. Using a GPU will make sure our model trains faster than using just a CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Are we using a GPU?\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "yD0_tpAIpS6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf2ee7e1-19ba-4206-ad15-d0a4768cc44f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep 28 09:31:07 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading and becoming one with the data"
      ],
      "metadata": {
        "id": "wkjUxBFKqWoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data (10% of 10 food classes from Food101)\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Download the data\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "\n",
        "# Instantiate zip file object\n",
        "zip_ref = ZipFile(\"10_food_classes_10_percent.zip\")\n",
        "# Unzip the download file\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmkJgmX9qbNV",
        "outputId": "4a05628f-a0c8-49e1-d449-c6782d4e2ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-28 09:31:08--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.194.128, 142.251.10.128, 142.251.12.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.194.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: â€˜10_food_classes_10_percent.zipâ€™\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M  81.3MB/s    in 2.0s    \n",
            "\n",
            "2022-09-28 09:31:10 (81.3 MB/s) - â€˜10_food_classes_10_percent.zipâ€™ saved [168546183/168546183]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How many images in each folder\n",
        "import os\n",
        "\n",
        "# Walkthrough 10 percent data directory and list number of files\n",
        "for dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):\n",
        "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}' . \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPHTGHYusdd3",
        "outputId": "16b7e1f9-5cfa-452b-aa5c-67e27cfcb6e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 directories and 0 images in '10_food_classes_10_percent' . \n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/train' . \n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi' . \n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings' . \n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry' . \n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza' . \n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger' . \n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon' . \n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice' . \n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak' . \n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream' . \n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen' . \n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/test' . \n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi' . \n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings' . \n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry' . \n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza' . \n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger' . \n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon' . \n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice' . \n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak' . \n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream' . \n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen' . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating data loaders (preparing the data)\n",
        "We'll use `ImageDataGenerator` class to load in our images in batches"
      ],
      "metadata": {
        "id": "qqzobqReqYMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup data inputs\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Varible with UpperCase = Hyperparamter\n",
        "IMG_SHAPE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dir = \"10_food_classes_10_percent/train/\"\n",
        "test_dir = \"10_food_classes_10_percent/test/\"\n",
        "\n",
        "print(\"Training images:\")\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "train_data_10_percent = train_datagen.flow_from_directory(train_dir,\n",
        "                                               target_size=IMG_SHAPE,\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               class_mode=\"categorical\",\n",
        "                                               seed=42)\n",
        "\n",
        "print(\"Testing Images\")\n",
        "test_data = test_datagen.flow_from_directory(test_dir,\n",
        "                                             target_size=IMG_SHAPE,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             class_mode=\"categorical\",\n",
        "                                             seed=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gzGXfrkprFK",
        "outputId": "6c50995c-143b-4a76-c449-66fdf435fb26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images:\n",
            "Found 750 images belonging to 10 classes.\n",
            "Testing Images\n",
            "Found 2500 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up callbacks (things to run whilst our model trains)\n",
        "\n",
        "Callbacks are extra functionality you can add to your model to be performed during or after training. Some of the most popular callbacks:\n",
        "\n",
        "* Tracking experiment with TensorBoard callback\n",
        "* Model checkpoint with ModelCheckpoint callback\n",
        "* Stopping a model from training (before it trains too long and overfits) with the EarlyStopping callbacks"
      ],
      "metadata": {
        "id": "uHikFtONwrUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow"
      ],
      "metadata": {
        "id": "sGEcIhYw5eOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TensorBoard callback (functionalize it because we need to create new one for each model)\n",
        "\n",
        "# import datetime class\n",
        "from datetime import datetime\n",
        "\n",
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.now().strftime(\"%Y%m%d%-%H%M\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "\n",
        "  print(f\"Saving tensorboard log files to : {log_dir}\")\n",
        "  return tensorboard_callback"
      ],
      "metadata": {
        "id": "bjjt4me5xn_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”‘ **Note:** You can customize the directory where your TensorBoard logs (model training metrics) get saved to whatever you like. The `log_dir` parameter we've created above is only one option"
      ],
      "metadata": {
        "id": "fDalcpStqIjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating feature extraction models with TensorFlow Hub\n",
        "\n",
        "In the past we've used TensorFlow to create our own models layer by layer from scratch.\n",
        "\n",
        "Now we're going to do a similar process, except the majority of our model's layers are going to come from [TensorFlow Hub](https://tfhub.dev/).\n",
        "\n",
        "In fact, we're going to use two models from TensorFlow Hub:\n",
        "1. [ResNetV2](https://arxiv.org/abs/1603.05027) -  a state of the art computer vision model architecture from 2016.\n",
        "2. [EfficientNet](https://arxiv.org/abs/1905.11946) - a state of the art computer vision architecture from 2019.\n",
        "\n",
        "State of the art means that at some point, both of these models have achieved the lowest error rate on [ImageNet (ILSVRC-2012-CLS)](http://www.image-net.org/), the gold standard of computer vision benchmarks.\n"
      ],
      "metadata": {
        "id": "daA_xFYtpbka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Let's compare the following two models\n",
        "resnet_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5\"\n",
        "\n",
        "efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\""
      ],
      "metadata": {
        "id": "I_mYIWxxpfAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import depedencies\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "T20wdBqL2Abi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/04-transfer-learning-feature-extraction.png)\n",
        "*What we're working towards building. Taking a pre-trained model and adding our own custom layers on top, extracting all of the underlying patterns learned on another dataset our own images.*"
      ],
      "metadata": {
        "id": "dXDubhflDbc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's make create_model() function to create a model from a URL\n",
        "def create_model(model_url, activation=\"softmax\", num_classes=10):\n",
        "  \"\"\"\n",
        "  Takes TensorFlow Hub URL and creates a keras Sequential model with it.\n",
        "\n",
        "  Args:\n",
        "    model_url (str): A TensorFlow Hub feature extraction URL.\n",
        "    num_classes : Number of output neurons in the output layer,\n",
        "      should be equal to number of target classes, default 10.\n",
        "\n",
        "  Returns:\n",
        "    An uncompiled Keras Sequential model with model_url as feature extractor layer\n",
        "    and dense output layer with num_classes output neurons.\n",
        "  \"\"\"\n",
        "  # Download the preatrained model and turn it into a Keras layer\n",
        "  feature_extractor_layer = hub.KerasLayer(model_url,\n",
        "                                           trainable=False, # freeze the already learned patterns\n",
        "                                           name=\"feature_extraction_layer\",\n",
        "                                           input_shape=IMG_SHAPE+(3, ) ) # fancy way to turn (224,224) to (224,224,3)\n",
        "  # Conditioning for binary problem\n",
        "  if activation == \"sigmoid\":\n",
        "    num_classes = 1\n",
        "\n",
        "  # Create our own model (adjust it to fit our own problem)\n",
        "  model = tf.keras.Sequential([\n",
        "    feature_extractor_layer,\n",
        "    tf.keras.layers.Dense(num_classes, activation=activation, name=\"output_layer\")\n",
        "  ])\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "tLXQ8S_r2cJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating and testing ResNet Feature Extraction model"
      ],
      "metadata": {
        "id": "8gem-VrJ-kdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Resnet model\n",
        "resnet_model = create_model(resnet_url,\n",
        "                            num_classes=train_data_10_percent.num_classes)"
      ],
      "metadata": {
        "id": "Lacwzt_g9LC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model.compile(loss=\"categorical_crossentropy\",\n",
        "                     optimizer=tf.keras.optimizers.Adam(),\n",
        "                     metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "ghx0-iyYAHY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in resnet_model.layers[0]:\n",
        "  print(layer.name)"
      ],
      "metadata": {
        "id": "TDGz0WnQAgTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Resnet model to our train_data-10_percent and use our TensorBoar callback\n",
        "resnet_history = resnet_model.fit(train_data_10_percent,\n",
        "                             epochs=5,\n",
        "                             steps_per_epoch=len(train_data_10_percent),\n",
        "                             validation_data= test_data,\n",
        "                             validation_steps=len(test_data),\n",
        "                             callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
        "                                                                    experiment_name=\"resnet50V2\")])\n",
        "\n"
      ],
      "metadata": {
        "id": "XRPWjBs_CmEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow!\n",
        "\n",
        "That. Is.Incredible. Our transfer learning feature extractor model  performed All the previous models we built by hand... (substainally) and in a quicker training time AND with only 10% of the training examples."
      ],
      "metadata": {
        "id": "jirPblRnTgSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boomm, looks how powerfull transfer learning it is. we've got 77% accuracy from our model that trained on just 10 percent of our data."
      ],
      "metadata": {
        "id": "FmQtq-LmkHVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "PZw7n7F6Yl9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create function to plot our loss curves...\n",
        "# Tidbid : you could put a function like this into script called \"helper.py\" and import it when you need it...\n",
        "def plot_loss_curves(history):\n",
        "  \"\"\"\n",
        "  Returns seperate loss curves for training and validation metrics\n",
        "\n",
        "  Args:\n",
        "    history: TensorFlow History object\n",
        "\n",
        "  Return:\n",
        "    Plot of training/validation loss and accuracy metrics.\n",
        "  \"\"\"\n",
        "\n",
        "  loss = history.history[\"loss\"]\n",
        "  val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "  accuracy = history.history[\"accuracy\"]\n",
        "  val_accuracy = history.history[\"val_accuracy\"]\n",
        "\n",
        "  epochs = range(len(history.history[\"loss\"]))\n",
        "\n",
        "  # Plot loss\n",
        "  plt.plot(epochs, loss, label=\"training_loss\")\n",
        "  plt.plot(epochs, val_loss, label=\"validation_loss\")\n",
        "\n",
        "  plt.title(\"loss\")\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot accuracy\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, accuracy, label=\"training_accuracy\")\n",
        "  plt.plot(epochs, val_accuracy, label=\"validation_accuracy\")\n",
        "\n",
        "  plt.title(\"accuracy\")\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "BOryaKWgkfC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(resnet_history)"
      ],
      "metadata": {
        "id": "AE067zXYXu-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating and testing EfficientNetB0  Feature Extraction model"
      ],
      "metadata": {
        "id": "dmYeUo4LhtMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create EfficientNetB0 feature extractor model - details on EfficientNet : https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html\n",
        "efficientnet_model = create_model(model_url=efficientnet_url,\n",
        "                                   num_classes=train_data_10_percent.num_classes)\n",
        "\n",
        "# Compile EfficientNet model\n",
        "efficientnet_model.compile(loss=\"categorical_crossentropy\",\n",
        "                           optimizer=tf.keras.optimizers.Adam(),\n",
        "                           metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the EfficientNet model to 10% of training data\n",
        "efficientnet_history = efficientnet_model.fit(train_data_10_percent,\n",
        "                                              epochs=5,\n",
        "                                              steps_per_epoch=len(train_data_10_percent),\n",
        "                                              validation_data=test_data,\n",
        "                                              validation_steps=len(test_data),\n",
        "                                              callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
        "                                                                                     experiment_name=\"efficientnetB0\")])"
      ],
      "metadata": {
        "id": "wNh3DE80iwEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems like our EfficientNetB0 is better than our ResnetV2 model. It got 85% accuracy with only 10% of train data"
      ],
      "metadata": {
        "id": "qKV3aGvSj6jE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(efficientnet_history)"
      ],
      "metadata": {
        "id": "-ljGSu_QkoIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efficientnet_model.summary()"
      ],
      "metadata": {
        "id": "l1cT8lDB_8rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model.summary()"
      ],
      "metadata": {
        "id": "Z7GaGcv8__Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing our model results using TensorBoard"
      ],
      "metadata": {
        "id": "cG3rFo4BB3p3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize our model with tensorboard\n",
        "\n",
        "# Load the tensorboard and visualize\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {\"tensorflow_hub\"}"
      ],
      "metadata": {
        "id": "aJtZnm0bPV1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the feature extraction model to classify image in Pen and Pencil dataset.\n"
      ],
      "metadata": {
        "id": "H0kQeeoBQOYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip our data\n",
        "from zipfile import ZipFile\n",
        "\n",
        "zip_reff = ZipFile(\"pen_pencil_images_04_challenge.zip\")\n",
        "zip_reff.extractall()\n",
        "zip_reff.close()"
      ],
      "metadata": {
        "id": "KqOHycWsXnFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the data by uploading from local computer"
      ],
      "metadata": {
        "id": "eGimVmzqvEFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing our data\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Set up random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Set the hyperparameter\n",
        "IMG_SHAPE = (224,224)\n",
        "\n",
        "# Instantiate ImageDataGen object\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "# Set up the directory\n",
        "train_dir = \"pen_pencil_images_04_challenge/train\"\n",
        "test_dir = \"pen_pencil_images_04_challenge/test\"\n",
        "\n",
        "# Import our data from directories and turn it into batches\n",
        "train_data = train_datagen.flow_from_directory(directory=train_dir,\n",
        "                                               target_size=IMG_SHAPE,\n",
        "                                               class_mode=\"binary\"\n",
        "                                               )\n",
        "\n",
        "test_data = test_datagen.flow_from_directory(directory=test_dir,\n",
        "                                             target_size=IMG_SHAPE,\n",
        "                                             class_mode=\"binary\",\n",
        "                                             )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F_r7OHMU56W",
        "outputId": "0b5e84b2-a5d1-41bf-9993-25588864bafd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20 images belonging to 2 classes.\n",
            "Found 4 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create EfficientNetB0 feature extractor model - details on EfficientNet : https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html\n",
        "resnet_model_2 = create_model(model_url=resnet_url,\n",
        "                                    activation=\"sigmoid\",\n",
        "                                    num_classes=train_data.num_classes)\n",
        "\n",
        "# Compile EfficientNet model\n",
        "resnet_model_2.compile(loss=\"binary_crossentropy\",\n",
        "                             optimizer=tf.keras.optimizers.Adam(),\n",
        "                             metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "BTcS5uX_fDaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What if we train it without the test data\n",
        "resnet_history_2= resnet_model_2.fit(train_data,\n",
        "                         epochs=10,\n",
        "                         steps_per_epoch=len(train_data),\n",
        "                         validation_data=test_data,\n",
        "                         validation_steps=len(test_data)\n",
        "                         )"
      ],
      "metadata": {
        "id": "ZHxLryF6PGXw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b0b0c32-33ff-4571-db3f-9793b9157443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.8931 - accuracy: 0.4000 - val_loss: 1.0244 - val_accuracy: 0.2500\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.7548 - accuracy: 0.5000 - val_loss: 0.9276 - val_accuracy: 0.2500\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.6370 - accuracy: 0.6000 - val_loss: 0.8455 - val_accuracy: 0.7500\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.5389 - accuracy: 0.7000 - val_loss: 0.7758 - val_accuracy: 0.7500\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4583 - accuracy: 0.8000 - val_loss: 0.7160 - val_accuracy: 0.7500\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.3921 - accuracy: 0.9000 - val_loss: 0.6637 - val_accuracy: 0.7500\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.3375 - accuracy: 0.9500 - val_loss: 0.6170 - val_accuracy: 0.7500\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.2921 - accuracy: 0.9500 - val_loss: 0.5746 - val_accuracy: 0.7500\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.2540 - accuracy: 0.9500 - val_loss: 0.5355 - val_accuracy: 0.7500\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.2219 - accuracy: 0.9500 - val_loss: 0.4993 - val_accuracy: 0.7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predicting on custom data\n",
        "To predict on custom data we need to:\n",
        "1. Preprocess the data to be the same format as the model was trained on\n",
        "2. Using our trained model to make prediction on"
      ],
      "metadata": {
        "id": "nVAS9IM2Dy7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get the subdirectories (these are our class names)\n",
        "import pathlib\n",
        "\n",
        "data_dir = pathlib.Path(train_dir) # acces the train dir\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob(\"*\")])) # get the train dir subdirectories name"
      ],
      "metadata": {
        "id": "e_1PRzrAcZwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Preprocess the data to be the same format as the model was trained on\n",
        "\n",
        "filename = \"/content/pen_pencil_images_04_challenge/test/pencil/ZJT4KKX1I09O.jpg\" # pencil image to make prediction on\n",
        "img_shape = 224\n"
      ],
      "metadata": {
        "id": "23HCsA3KEBQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Making prediction\n",
        "resnet_model_pred = resnet_model_2.predict(tf.expand_dims(img, axis=0)) # expand dims for the batch size\n",
        "resnet_model_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fz3BlLA1EoJX",
        "outputId": "ac3a52b3-c72b-4493-b718-19e60cf0c8c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.721054]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the predicition into label\n",
        "resnet_model_label = class_names[int(tf.round(resnet_model_pred))]\n",
        "resnet_model_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KBVv8-TjE5gc",
        "outputId": "3d32894f-ab20-4c41-9995-c1e7bdc9bcc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pencil'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}